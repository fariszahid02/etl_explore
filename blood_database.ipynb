{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa144c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ complete_donor already exists, will update incrementally\n",
      "Downloading daily_retention...\n",
      "✅ daily_retention downloaded\n",
      "Downloading daily_donor_rates...\n",
      "✅ daily_donor_rates downloaded\n",
      "Latest date in complete_donor: 2025-11-26\n",
      "❌ No data for 2025-11-27\n",
      "✅ Updated complete_donor saved with 0 new rows\n",
      "✅ Loaded daily_retention with 6914184 rows\n",
      "✅ Loaded daily_donor_rates with 643803 rows\n",
      "✅ Loaded daily_donor (2025-11-26) with 1446 rows\n",
      "Tables in database: [('complete_donor',), ('daily_donor',), ('daily_donor_rates',), ('daily_retention',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import duckdb\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "\n",
    "# Static URLs for other files\n",
    "files_info = {\n",
    "    \"complete_donor\": \"https://data.kijang.net/dea/donations/historical.parquet\",\n",
    "    \"daily_retention\": \"https://data.kijang.net/dea/retention/data.parquet\",\n",
    "    \"daily_donor_rates\": \"https://data.kijang.net/dea/donorrate/data.parquet\"\n",
    "}\n",
    "\n",
    "# Base URL for daily donor files\n",
    "base_url = \"https://data.kijang.net/dea/donations\"\n",
    "\n",
    "# Folder for downloaded files\n",
    "folder = \"downloaded_parquet\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "complete_donor_path = os.path.join(folder, \"complete_donor.parquet\")\n",
    "daily_donor_path = os.path.join(folder, \"daily_donor.parquet\")\n",
    "\n",
    "# Function to find latest available daily donor file\n",
    "\n",
    "def get_latest_daily_donor(base_url, max_days=30):\n",
    "    today = datetime.today()\n",
    "    for i in range(max_days):\n",
    "        date_to_try = today - timedelta(days=i)\n",
    "        date_str = date_to_try.strftime(\"%Y-%m-%d\")\n",
    "        url = f\"{base_url}/{date_str}.parquet\"\n",
    "        try:\n",
    "            response = requests.get(url, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                return url, date_str\n",
    "        except:\n",
    "            pass\n",
    "    return None, None\n",
    "\n",
    "# Step 1: Download static files (complete_donor only if not exists)\n",
    "if not os.path.exists(complete_donor_path):\n",
    "    print(f\"Downloading initial complete_donor from {files_info['complete_donor']}...\")\n",
    "    r = requests.get(files_info['complete_donor'], verify=False)\n",
    "    r.raise_for_status()\n",
    "    with open(complete_donor_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"✅ Initial complete_donor downloaded\")\n",
    "else:\n",
    "    print(\"✅ complete_donor already exists, will update incrementally\")\n",
    "\n",
    "# Always refresh daily_retention and daily_donor_rates\n",
    "for name in [\"daily_retention\", \"daily_donor_rates\"]:\n",
    "    path = os.path.join(folder, f\"{name}.parquet\")\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    print(f\"Downloading {name}...\")\n",
    "    r = requests.get(files_info[name], verify=False)\n",
    "    r.raise_for_status()\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"✅ {name} downloaded\")\n",
    "\n",
    "# Step 2: Connect to DuckDB and load complete_donor\n",
    "conn = duckdb.connect(\"donations.duckdb\")\n",
    "conn.execute(\"DROP TABLE IF EXISTS complete_donor\")\n",
    "conn.execute(f\"CREATE TABLE complete_donor AS SELECT * FROM read_parquet('{complete_donor_path}')\")\n",
    "\n",
    "# Find latest date in complete_donor\n",
    "latest_complete_date = conn.execute(\"SELECT MAX(visit_date) FROM complete_donor\").fetchone()[0]\n",
    "print(f\"Latest date in complete_donor: {latest_complete_date}\")\n",
    "\n",
    "# Step 3: Incrementally update complete_donor with daily files\n",
    "start_date = latest_complete_date + timedelta(days=1)\n",
    "today = datetime.today().date()\n",
    "new_rows_added = 0\n",
    "\n",
    "while start_date <= today:\n",
    "    date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    url = f\"{base_url}/{date_str}.parquet\"\n",
    "    try:\n",
    "        r = requests.get(url, verify=False)\n",
    "        if r.status_code == 200:\n",
    "            # Save temporary daily file\n",
    "            with open(daily_donor_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            # Append to complete_donor\n",
    "            conn.execute(f\"INSERT INTO complete_donor SELECT * FROM read_parquet('{daily_donor_path}')\")\n",
    "            count = conn.execute(\"SELECT COUNT(*) FROM read_parquet(?)\", [daily_donor_path]).fetchone()[0]\n",
    "            new_rows_added += count\n",
    "            print(f\"✅ Added {count} rows for {date_str}\")\n",
    "        else:\n",
    "            print(f\"❌ No data for {date_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for {date_str}: {e}\")\n",
    "    start_date += timedelta(days=1)\n",
    "\n",
    "# Step 4: Persist updated complete_donor back to Parquet\n",
    "conn.execute(f\"COPY complete_donor TO '{complete_donor_path}' (FORMAT 'parquet')\")\n",
    "print(f\"✅ Updated complete_donor saved with {new_rows_added} new rows\")\n",
    "\n",
    "# Step 5: Load other tables\n",
    "for name in [\"daily_retention\", \"daily_donor_rates\"]:\n",
    "    path = os.path.join(folder, f\"{name}.parquet\")\n",
    "    conn.execute(f\"DROP TABLE IF EXISTS {name}\")\n",
    "    conn.execute(f\"CREATE TABLE {name} AS SELECT * FROM read_parquet('{path}')\")\n",
    "    count = conn.execute(f\"SELECT COUNT(*) FROM {name}\").fetchone()[0]\n",
    "    print(f\"✅ Loaded {name} with {count} rows\")\n",
    "\n",
    "# Step 6: Load latest daily_donor (fallback logic)\n",
    "latest_url, latest_date = get_latest_daily_donor(base_url)\n",
    "if latest_url:\n",
    "    r = requests.get(latest_url, verify=False)\n",
    "    r.raise_for_status()\n",
    "    with open(daily_donor_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    conn.execute(\"DROP TABLE IF EXISTS daily_donor\")\n",
    "    conn.execute(f\"CREATE TABLE daily_donor AS SELECT * FROM read_parquet('{daily_donor_path}')\")\n",
    "    count = conn.execute(\"SELECT COUNT(*) FROM daily_donor\").fetchone()[0]\n",
    "    print(f\"✅ Loaded daily_donor ({latest_date}) with {count} rows\")\n",
    "else:\n",
    "    print(\"❌ Could not find any daily donor file in last 30 days\")\n",
    "\n",
    "# Show tables\n",
    "print(\"Tables in database:\", conn.execute(\"SHOW TABLES\").fetchall())\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481bb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
